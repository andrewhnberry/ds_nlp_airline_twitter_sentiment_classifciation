{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project will explore the popular Kaggle dataset of airline twitter sentiment, with the goal of using various sentiment anlaysis and classifcations methodology in order to find the highest accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fist let us import some of the packages we will be using \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remove some of the columns we will not be using from this dataframe.\n",
    "\n",
    "##### Such as: \n",
    "1. tweet_id\n",
    "2. name\n",
    "\n",
    "All the other columns will be helpful in our classification methodologies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('tweet_id',axis = 1)\n",
    "df = df.drop('name', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploritory analysis \n",
    "\n",
    "Let's create some basic visualizations and see what we know about this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This tells us there is 14,640 tweets in this data set\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Improve plot to show various counts\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.countplot(x = df['airline_sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the postive tweets\n",
    "# Lets create a dataframe with only positive tweets\n",
    "p_df = df[df['airline_sentiment'].str.contains('positive')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delta</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US Airways</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          airline  airline_sentiment_confidence\n",
       "2       Southwest                           570\n",
       "1           Delta                           544\n",
       "4          United                           492\n",
       "0        American                           336\n",
       "3      US Airways                           269\n",
       "5  Virgin America                           152"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at the top airlines based on amount of positive tweets\n",
    "top_airline = p_df[['airline','airline_sentiment_confidence']]\n",
    "top_airline = top_airline.groupby('airline', as_index = False).count() \n",
    "top_airline.sort_values('airline_sentiment_confidence', ascending = False)\n",
    "\n",
    "# We can see that Southwest airlines as the highest number of positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United</td>\n",
       "      <td>2633</td>\n",
       "      <td>2633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US Airways</td>\n",
       "      <td>2263</td>\n",
       "      <td>2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American</td>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>1186</td>\n",
       "      <td>1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delta</td>\n",
       "      <td>955</td>\n",
       "      <td>955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          airline  airline_sentiment_confidence  negativereason\n",
       "4          United                          2633            2633\n",
       "3      US Airways                          2263            2263\n",
       "0        American                          1960            1960\n",
       "2       Southwest                          1186            1186\n",
       "1           Delta                           955             955\n",
       "5  Virgin America                           181             181"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets do the same but for negative tweets, which airlines are commited the worse offence\n",
    "# to passengers'\n",
    "\n",
    "n_df = df[df['airline_sentiment'].str.contains('negative')]\n",
    "\n",
    "\n",
    "bad_airline = n_df[['airline','airline_sentiment_confidence','negativereason']]\n",
    "bad_airline = bad_airline.groupby('airline', as_index = False).count()\n",
    "bad_airline.sort_values('airline_sentiment_confidence', ascending = False)\n",
    "\n",
    "#Seems as if United has been pretty naughty, as well as US airways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should look at which airlines has the highest percentage of tweets that are positive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negativereason</th>\n",
       "      <th>airline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>2910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Late Flight</td>\n",
       "      <td>1665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cancelled Flight</td>\n",
       "      <td>847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lost Luggage</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flight Booking Problems</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Flight Attendant Complaints</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longlines</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Damaged Luggage</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                negativereason  airline\n",
       "3       Customer Service Issue     2910\n",
       "7                  Late Flight     1665\n",
       "1                   Can't Tell     1190\n",
       "2             Cancelled Flight      847\n",
       "8                 Lost Luggage      724\n",
       "0                   Bad Flight      580\n",
       "6      Flight Booking Problems      529\n",
       "5  Flight Attendant Complaints      481\n",
       "9                    longlines      178\n",
       "4              Damaged Luggage       74"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets expand a bit more on the negativereason column, and see what types of complaints\n",
    "# consmers had \n",
    "\n",
    "complaints_df = n_df[['airline','negativereason']]\n",
    "complaints_df = complaints_df.groupby('negativereason', as_index = False).count()\n",
    "complaints_df.sort_values('airline', ascending = False)\n",
    "\n",
    "# we can see that Customer Service Issue and Late flights took the bulk of it\n",
    "#interesing to see what \"Can't Tell\" is, overall just a bad experience. \n",
    "#tbh i have to admit, american based carriers truly arn't the best relative to the world\n",
    "#in terms of service. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do to NLP work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us import our NLP plugins, lets break it down differnetly for NLTK, and Spacey \n",
    "\n",
    "## MORE WORK NEEDED\n",
    "# INtorduce SPACY VERSION\n",
    "# introduce a stemmer\n",
    "# clean up the code\n",
    "# DO sentiment anlaysis, and compare it to what was labeled, see if its the same or different with NLTK version \n",
    "# Plus spacey version\n",
    "# Plus Gensim see the difference\n",
    "\n",
    "# NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "#Adds stuff to our stop wors list\n",
    "stop_words.extend([\"@\",\"n't\",'.',','])\n",
    "\n",
    "\n",
    "def WN_lemmatizer_NLTK(token):\n",
    "    \n",
    "    return WordNetLemmatizer().lemmatize(token)\n",
    "\n",
    "def tokenizer_NLTK_1(text):\n",
    "    #simple version\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def tokenizer_NLTK_2(text):\n",
    "    #a bit more complex\n",
    "    #Tokenizes\n",
    "    the_tokens = nltk.word_tokenize(text)\n",
    "    #Removes Stop_WOrds\n",
    "    the_tokens = [token for token in the_tokens if token not in stop_words]\n",
    "    \n",
    "    #Lemmatized the word too\n",
    "    for i in the_tokens:\n",
    "        WN_lemmatizer_NLTK(i)\n",
    "        \n",
    "    #the_tokens = [WN_lemmatizer_NLTK for i in the_tokens]    \n",
    "    \n",
    "    return the_tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to do a sentiment anlaysis we need to tokenize our text data\n",
    "\n",
    "# We need to put our text data in list form\n",
    "\n",
    "tweet_text_list = []\n",
    "\n",
    "for i in df['text']:\n",
    "    \n",
    "    tweet_text_list.append(tokenizer_NLTK_2(i))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we completed the pre_processing, we can move on to do some classification work :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import some mchine learning plugins\n",
    "\n",
    "# We go to split our data into train and test, for our classifiers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some evaluation metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Classifcation Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We need to establish our dependent variable \n",
    "y = df.airline_sentiment\n",
    "# The independent variable \n",
    "X = df.text\n",
    "\n",
    "def convert_sentiment_to_integer(word_sentiment):\n",
    "    \n",
    "    return {'negative': 0, 'neutral':1, 'positive': 2}[word_sentiment]\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer = 'word', ngram_range=(1,2))\n",
    "\n",
    "X = \n",
    "\n",
    "\n",
    "split = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '@JetBlue  I did.  They have no idea where it is.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-e1b8a46080bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDT_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mDT_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDT_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'M8[ns]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '@JetBlue  I did.  They have no idea where it is.'"
     ]
    }
   ],
   "source": [
    "\n",
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "DT_model.fit(X_train, y_train)\n",
    "\n",
    "DT_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "   \n",
    "#Get the data:\n",
    "boston = load_boston()\n",
    "#The dependent variable\n",
    "y = boston.target\n",
    "#The dependent variable\n",
    "X = boston.data\n",
    "\n",
    "#Split the data so the train set contains 60% of the points\n",
    "split = 0.40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
